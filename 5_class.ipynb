{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "5_class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nausheen53/SMAI/blob/master/5_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmFlgNzbXCY0",
        "colab_type": "code",
        "outputId": "6781e7a0-76a8-435f-96f9-92f030a06329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlJm2qYCW9qN",
        "colab_type": "code",
        "outputId": "ae6cbdd3-7559-44b8-eb45-122787e81ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import os\n",
        "import ssl\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use(\"ggplot\")\n",
        "from sklearn import svm\n",
        "import textblob\n",
        "from textblob import TextBlob, Word\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hETldMfW9qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AuthorClassifier:\n",
        "    labels_of_train=[]\n",
        "    labels_of_test=[]\n",
        "    final_list_train=[]\n",
        "    final_list_test=[]\n",
        "    \n",
        "    def train(self,path):\n",
        "        text_of_train = self.load(path)\n",
        "        self.clean_data(text_of_train)\n",
        "        \n",
        "        \n",
        "    def load(self,path):\n",
        "        df = pd.read_csv(path)\n",
        "        print(df)\n",
        "        train=df.sample(frac=0.8,random_state=200)\n",
        "        train = np.array(train)\n",
        "        self.labels_of_train=[]\n",
        "        for row in train:\n",
        "            self.labels_of_train.append(row[2])\n",
        "        text_of_train = []\n",
        "        for row in train:\n",
        "            text_of_train.append(row[1])\n",
        "        return text_of_train\n",
        "   \n",
        "    def clean_data(self,text_of_train):\n",
        "        import string\n",
        "        from collections import Counter\n",
        "        self.final_list_train=[]\n",
        "        for sent in text_of_train:\n",
        "            # print(\"1\")\n",
        "            sent = sent.lower()\n",
        "            sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "            # print(\"sent1 \",sent)\n",
        "            from nltk.tokenize import word_tokenize\n",
        "            tokens = word_tokenize(sent)\n",
        "            # remove all tokens that are not alphabetic\n",
        "            words = [word for word in tokens if word.isalpha()]\n",
        "            #remove stopwords\n",
        "            from nltk.corpus import stopwords\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            words = [w for w in words if not w in stop_words]\n",
        "            sent=\" \".join(words)\n",
        "            # print(\"sentttt \",sent)\n",
        "            self.final_list_train.append(sent)\n",
        "\n",
        "            \n",
        "    def vectorise_test_train(self):\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.preprocessing import normalize\n",
        "        text = self.final_list_train\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vectorizer.fit(text)\n",
        "        vector = vectorizer.fit_transform(text)\n",
        "        vector = vector.todense()\n",
        "        vector1 = vectorizer.transform(self.final_list_test)\n",
        "        vector1 = vector1.todense()\n",
        "        words = vectorizer.get_feature_names()\n",
        "        vector_norm_train = normalize(vector)\n",
        "        return vector,vector1\n",
        "        \n",
        "\n",
        "    \n",
        "    def predict(self,path):\n",
        "        text_of_test = self.load_test(path)\n",
        "        self.clean_data_test(text_of_test)\n",
        "        vector,vector1 =self.vectorise_test_train()\n",
        "        prediction=self.classify(vector,vector1)\n",
        "        return prediction\n",
        "        \n",
        "        \n",
        "        \n",
        "    def load_test(self,path):\n",
        "        df = pd.read_csv(path)\n",
        "        test=df.sample(frac=0.8,random_state=200)\n",
        "        test = np.array(test)\n",
        "        self.labels_of_test=[]\n",
        "        for row in test:\n",
        "            self.labels_of_test.append(row[2])\n",
        "        text_of_test = []\n",
        "        for row in test:\n",
        "            text_of_test.append(row[1])\n",
        "        return text_of_test \n",
        "    \n",
        "    def clean_data_test(self,text_of_test):\n",
        "        import string\n",
        "        from collections import Counter\n",
        "        self.final_list_test=[]\n",
        "        for sent in text_of_test:\n",
        "            # print(\"1\")\n",
        "            sent = sent.lower()\n",
        "            sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "            # print(\"sent1 \",sent)\n",
        "            from nltk.tokenize import word_tokenize\n",
        "            tokens = word_tokenize(sent)\n",
        "            # remove all tokens that are not alphabetic\n",
        "            words = [word for word in tokens if word.isalpha()]\n",
        "            #remove stopwords\n",
        "            from nltk.corpus import stopwords\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            words = [w for w in words if not w in stop_words]\n",
        "            sent=\" \".join(words)\n",
        "            # print(\"sentttt \",sent)\n",
        "            self.final_list_test.append(sent)\n",
        "        \n",
        "    \n",
        "    def classify(self,vector,vector1):\n",
        "        from sklearn.svm import LinearSVC\n",
        "        clf = LinearSVC(random_state=0, tol=1e-5,max_iter=5000,C=1.0)\n",
        "        clf.fit(vector,self.labels_of_train)\n",
        "        # print(clf.predict(test))\n",
        "        predicted_label=clf.predict(vector1)\n",
        "        accu_score = accuracy_score(self.labels_of_test, predicted_label,normalize=True)\n",
        "        print(accu_score) \n",
        "        f11 = f1_score(self.labels_of_test, predicted_label,average=None)\n",
        "        re = recall_score(self.labels_of_test, predicted_label,average='weighted')\n",
        "        cm = confusion_matrix(self.labels_of_test, predicted_label)\n",
        "        pr_sc = precision_score(self.labels_of_test, predicted_label,average='weighted')\n",
        "        clrp = classification_report(self.labels_of_test, predicted_label)\n",
        "        print(\"**********LINEAR SVC WITH C= 1.0 ***********\")\n",
        "        print(\"accuracy score \",accu_score)\n",
        "        print(\"********************\")\n",
        "        print(\"f1 score \",f11)\n",
        "        print(\"********************\")\n",
        "        print(\"recall score \",re)\n",
        "        print(\"********************\")\n",
        "        print(\"confusion matrix \",cm)\n",
        "        print(\"********************\")\n",
        "        print(\"precision score \",pr_sc)\n",
        "        print(\"********************\")\n",
        "        print(\"classification report \",clrp)\n",
        "        return predicted_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boi8QHmDW9qc",
        "colab_type": "code",
        "outputId": "fa54324e-de16-4b1b-d8cb-eb4d84f088db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "auth_classifier = AuthorClassifier()\n",
        "auth_classifier.train('/content/drive/My Drive/Train.csv') # Path to the train.csv will be provided\n",
        "predictions = auth_classifier.predict('/content/drive/My Drive/Train.csv') \n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Unnamed: 0                                               text author\n",
            "0               0  The sight of the military restored hope to tho...    MWS\n",
            "1               1  Just as the building was wiped out by a German...    HPL\n",
            "2               2                    Do you wonder how it will seem?    HPL\n",
            "3               3  My revenge is of no moment to you; yet, while ...    MWS\n",
            "4               4  If he dares to reply, you can tell him from Lu...    EAP\n",
            "...           ...                                                ...    ...\n",
            "15658       15658                         Get bald, too, very young.    HPL\n",
            "15659       15659  I performed the first part of my journey on ho...    MWS\n",
            "15660       15660  He had a narrow head, bulging, watery blue eye...    HPL\n",
            "15661       15661  There does not exist the man in England with a...    MWS\n",
            "15662       15662  \"Every man,\" he said, \"dreams about something,...    MWS\n",
            "\n",
            "[15663 rows x 3 columns]\n",
            "0.9898643256185156\n",
            "**********LINEAR SVC WITH C= 1.0 ***********\n",
            "accuracy score  0.9898643256185156\n",
            "********************\n",
            "f1 score  [0.98928359 0.99261118 0.98809066]\n",
            "********************\n",
            "recall score  0.9898643256185156\n",
            "********************\n",
            "confusion matrix  [[4985   10   30]\n",
            " [  24 3560    6]\n",
            " [  44   13 3858]]\n",
            "********************\n",
            "precision score  0.9898753173668947\n",
            "********************\n",
            "classification report                precision    recall  f1-score   support\n",
            "\n",
            "         EAP       0.99      0.99      0.99      5025\n",
            "         HPL       0.99      0.99      0.99      3590\n",
            "         MWS       0.99      0.99      0.99      3915\n",
            "\n",
            "    accuracy                           0.99     12530\n",
            "   macro avg       0.99      0.99      0.99     12530\n",
            "weighted avg       0.99      0.99      0.99     12530\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPU-y8UXZWtP",
        "colab_type": "code",
        "outputId": "a919f966-4d12-4d38-f370-f1c286bd402f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['EAP' 'EAP' 'EAP' ... 'MWS' 'MWS' 'HPL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WrREFH1ZZ1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}