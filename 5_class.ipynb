{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "5_class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nausheen53/SMAI/blob/master/5_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmFlgNzbXCY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63211fb1-6905-4911-a8b6-1e831916b1cc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlJm2qYCW9qN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7735f6a1-b8b7-4b97-f69e-c7ee9c888d17"
      },
      "source": [
        "import os\n",
        "import ssl\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use(\"ggplot\")\n",
        "from sklearn import svm\n",
        "import textblob\n",
        "from textblob import TextBlob, Word\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hETldMfW9qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AuthorClassifier:\n",
        "    labels_of_train=[]\n",
        "    labels_of_test=[]\n",
        "    final_list_train=[]\n",
        "    final_list_test=[]\n",
        "    \n",
        "    def train(self,path):\n",
        "        text_of_train = self.load(path)\n",
        "        self.clean_data(text_of_train)\n",
        "        \n",
        "        \n",
        "    def load(self,path):\n",
        "        df = pd.read_csv(path,header = None)\n",
        "        print(df)\n",
        "        train=df.sample(frac=0.8,random_state=200)\n",
        "        train = np.array(train)\n",
        "        self.labels_of_train=[]\n",
        "        for row in train:\n",
        "            self.labels_of_train.append(row[2])\n",
        "        text_of_train = []\n",
        "        for row in train:\n",
        "            text_of_train.append(row[1])\n",
        "        return text_of_train\n",
        "   \n",
        "    def clean_data(self,text_of_train):\n",
        "        import string\n",
        "        from collections import Counter\n",
        "        self.final_list_train=[]\n",
        "        for sent in text_of_train:\n",
        "            # print(\"1\")\n",
        "            sent = sent.lower()\n",
        "            sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "            # print(\"sent1 \",sent)\n",
        "            from nltk.tokenize import word_tokenize\n",
        "            tokens = word_tokenize(sent)\n",
        "            # remove all tokens that are not alphabetic\n",
        "            words = [word for word in tokens if word.isalpha()]\n",
        "            #remove stopwords\n",
        "            from nltk.corpus import stopwords\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            words = [w for w in words if not w in stop_words]\n",
        "            sent=\" \".join(words)\n",
        "            # print(\"sentttt \",sent)\n",
        "            self.final_list_train.append(sent)\n",
        "\n",
        "            \n",
        "    def vectorise_test_train(self):\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.preprocessing import normalize\n",
        "        text = self.final_list_train\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vectorizer.fit(text)\n",
        "        vector = vectorizer.fit_transform(text)\n",
        "        vector = vector.todense()\n",
        "        vector1 = vectorizer.transform(self.final_list_test)\n",
        "        vector1 = vector1.todense()\n",
        "        words = vectorizer.get_feature_names()\n",
        "        vector_norm_train = normalize(vector)\n",
        "        return vector,vector1\n",
        "        \n",
        "\n",
        "    \n",
        "    def predict(self,path):\n",
        "        text_of_test = self.load_test(path)\n",
        "        self.clean_data_test(text_of_test)\n",
        "        vector,vector1 =self.vectorise_test_train()\n",
        "        prediction=self.classify(vector,vector1)\n",
        "        return prediction\n",
        "        \n",
        "        \n",
        "        \n",
        "    def load_test(self,path):\n",
        "        df = pd.read_csv(path,header = 0)\n",
        "        test=df.sample(frac=0.8,random_state=200)\n",
        "        test = np.array(test)\n",
        "        self.labels_of_test=[]\n",
        "        for row in test:\n",
        "            self.labels_of_test.append(row[2])\n",
        "        text_of_test = []\n",
        "        for row in test:\n",
        "            text_of_test.append(row[1])\n",
        "        return text_of_test \n",
        "    \n",
        "    def clean_data_test(self,text_of_test):\n",
        "        import string\n",
        "        from collections import Counter\n",
        "        self.final_list_test=[]\n",
        "        for sent in text_of_test:\n",
        "            # print(\"1\")\n",
        "            sent = sent.lower()\n",
        "            sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "            # print(\"sent1 \",sent)\n",
        "            from nltk.tokenize import word_tokenize\n",
        "            tokens = word_tokenize(sent)\n",
        "            # remove all tokens that are not alphabetic\n",
        "            words = [word for word in tokens if word.isalpha()]\n",
        "            #remove stopwords\n",
        "            from nltk.corpus import stopwords\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            words = [w for w in words if not w in stop_words]\n",
        "            sent=\" \".join(words)\n",
        "            # print(\"sentttt \",sent)\n",
        "            self.final_list_test.append(sent)\n",
        "        \n",
        "    \n",
        "    def classify(self,vector,vector1):\n",
        "        from sklearn.svm import LinearSVC\n",
        "        clf = LinearSVC(random_state=0, tol=1e-5,max_iter=5000,C=1.0)\n",
        "        clf.fit(vector,self.labels_of_train)\n",
        "        # print(clf.predict(test))\n",
        "        predicted_label=clf.predict(vector1)\n",
        "        accu_score = accuracy_score(self.labels_of_test, predicted_label,normalize=True)\n",
        "        print(accu_score) \n",
        "        f11 = f1_score(self.labels_of_test, predicted_label,average=None)\n",
        "        re = recall_score(self.labels_of_test, predicted_label,average='weighted')\n",
        "        cm = confusion_matrix(self.labels_of_test, predicted_label)\n",
        "        pr_sc = precision_score(self.labels_of_test, predicted_label,average='weighted')\n",
        "        clrp = classification_report(self.labels_of_test, predicted_label)\n",
        "        print(\"**********LINEAR SVC WITH C= 1.0 ***********\")\n",
        "        print(\"accuracy score \",accu_score)\n",
        "        print(\"********************\")\n",
        "        print(\"f1 score \",f11)\n",
        "        print(\"********************\")\n",
        "        print(\"recall score \",re)\n",
        "        print(\"********************\")\n",
        "        print(\"confusion matrix \",cm)\n",
        "        print(\"********************\")\n",
        "        print(\"precision score \",pr_sc)\n",
        "        print(\"********************\")\n",
        "        print(\"classification report \",clrp)\n",
        "        return predicted_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boi8QHmDW9qc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "56b65b31-2163-4e93-993e-2410461b44b4"
      },
      "source": [
        "auth_classifier = AuthorClassifier()\n",
        "auth_classifier.train('/content/drive/My Drive/Train.csv') # Path to the train.csv will be provided\n",
        "predictions = auth_classifier.predict('/content/drive/My Drive/Train.csv') \n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             0                                                  1       2\n",
            "0          NaN                                               text  author\n",
            "1          0.0  The sight of the military restored hope to tho...     MWS\n",
            "2          1.0  Just as the building was wiped out by a German...     HPL\n",
            "3          2.0                    Do you wonder how it will seem?     HPL\n",
            "4          3.0  My revenge is of no moment to you; yet, while ...     MWS\n",
            "...        ...                                                ...     ...\n",
            "15659  15658.0                         Get bald, too, very young.     HPL\n",
            "15660  15659.0  I performed the first part of my journey on ho...     MWS\n",
            "15661  15660.0  He had a narrow head, bulging, watery blue eye...     HPL\n",
            "15662  15661.0  There does not exist the man in England with a...     MWS\n",
            "15663  15662.0  \"Every man,\" he said, \"dreams about something,...     MWS\n",
            "\n",
            "[15664 rows x 3 columns]\n",
            "0.958659217877095\n",
            "**********LINEAR SVC WITH C= 1.0 ***********\n",
            "accuracy score  0.958659217877095\n",
            "********************\n",
            "f1 score  [0.95817265 0.9616404  0.9565441 ]\n",
            "********************\n",
            "recall score  0.958659217877095\n",
            "********************\n",
            "confusion matrix  [[4845   69  111]\n",
            " [ 111 3447   32]\n",
            " [ 132   63 3720]]\n",
            "********************\n",
            "precision score  0.9587133452204023\n",
            "********************\n",
            "classification report                precision    recall  f1-score   support\n",
            "\n",
            "         EAP       0.95      0.96      0.96      5025\n",
            "         HPL       0.96      0.96      0.96      3590\n",
            "         MWS       0.96      0.95      0.96      3915\n",
            "\n",
            "    accuracy                           0.96     12530\n",
            "   macro avg       0.96      0.96      0.96     12530\n",
            "weighted avg       0.96      0.96      0.96     12530\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPU-y8UXZWtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a674593-966e-48a5-e4f3-188deb4e38d1"
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['EAP' 'EAP' 'EAP' ... 'EAP' 'EAP' 'EAP']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WrREFH1ZZ1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}