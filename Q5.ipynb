{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wEEV0Fgvbr05bCQRz0n7houvHqY-vfao",
      "authorship_tag": "ABX9TyMOHsnFGFvZQK75kUqnxTbq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nausheen53/SMAI/blob/master/Q5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFth3TKTYdjy",
        "colab_type": "code",
        "outputId": "1c1f9b83-5f8d-4f4b-db0e-f31e368260ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mtMPAJSc4Ew",
        "colab_type": "code",
        "outputId": "d36a9e2a-3bc6-4097-87fa-d87b6d76e944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import os\n",
        "import ssl\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use(\"ggplot\")\n",
        "from sklearn import svm\n",
        "import textblob\n",
        "from textblob import TextBlob, Word\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p3zWZBkn0As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4vmAQr3c-7B",
        "colab_type": "code",
        "outputId": "f6a37fcb-3e2d-474c-d91b-ac2277e8cd24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Train.csv',header = 0)\n",
        "print(len(df))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "110wh3Rlecge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
        "test=df.drop(train.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQWbtAKGegQF",
        "colab_type": "code",
        "outputId": "1bc0e79c-119b-48cc-f02a-e2202d9959be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train = np.array(train)\n",
        "test = np.array(test)\n",
        "labels_of_train=[]\n",
        "for row in train:\n",
        "    labels_of_train.append(row[2])\n",
        "text_of_train = []\n",
        "for row in train:\n",
        "    text_of_train.append(row[1])\n",
        "print(len(labels_of_train))    \n",
        "\n",
        "labels_of_test=[]\n",
        "for row in test:\n",
        "    labels_of_test.append(row[2])\n",
        "text_of_test = []\n",
        "for row in test:\n",
        "    text_of_test.append(row[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZwQ6szvelm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "final_list_train=[]\n",
        "for sent in text_of_train:\n",
        "    # print(\"1\")\n",
        "    sent = sent.lower()\n",
        "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    # print(\"sent1 \",sent)\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    tokens = word_tokenize(sent)\n",
        "    # remove all tokens that are not alphabetic\n",
        "    words = [word for word in tokens if word.isalpha()]\n",
        "    #remove stopwords\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    sent=\" \".join(words)\n",
        "    # print(\"sentttt \",sent)\n",
        "    final_list_train.append(sent)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDWHJsQTfVMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "final_list_test=[]\n",
        "for sent in text_of_test:\n",
        "    # print(\"1\")\n",
        "    sent = sent.lower()\n",
        "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    # print(\"senttttt \",sent)\n",
        "#     tokens= sent.split()\n",
        "    ###################################################\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    tokens = word_tokenize(sent)\n",
        "    # remove all tokens that are not alphabetic\n",
        "    words = [word for word in tokens if word.isalpha()]\n",
        "    #remove stopwords\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    sent=\" \".join(words)\n",
        "    final_list_test.append(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cps3Rezdfj7S",
        "colab_type": "code",
        "outputId": "564b299f-0e6e-4b27-df5b-fd49353abedf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1_fjWutpCja",
        "colab_type": "code",
        "outputId": "fb07cb49-72c3-45a1-a0d4-df00447b7621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.vector_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEQdrBlCvDJW",
        "colab_type": "code",
        "outputId": "0e11fef9-488a-4a1b-9124-e7b25c09ee6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "text = final_list_train\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(text)\n",
        "vector = vectorizer.fit_transform(text)\n",
        "vector = vector.todense()\n",
        "vector1 = vectorizer.transform(final_list_test)\n",
        "vector1 = vector1.todense()\n",
        "words = vectorizer.get_feature_names()\n",
        "# print(words)\n",
        "vector_norm_train = normalize(vector)\n",
        "print(vector_norm_train)\n",
        "print(vector_norm_train.shape)\n",
        "\n",
        "clf = svm.SVC(kernel='linear', C = 4.0)\n",
        "print((vector_norm_train).shape)\n",
        "print(len(labels_of_train))\n",
        "clf.fit(vector,labels_of_train)\n",
        "print(clf.predict(vector1))\n",
        "predicted_label=clf.predict(vector1)    \n",
        "accu_score = accuracy_score(labels_of_test, predicted_label,normalize=True)\n",
        "print(accu_score)  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(12530, 21283)\n",
            "(12530, 21283)\n",
            "12530\n",
            "['EAP' 'MWS' 'MWS' ... 'EAP' 'HPL' 'MWS']\n",
            "0.7867858282796042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIfAZYrI4AIL",
        "colab_type": "code",
        "outputId": "e937e7e6-3f83-4672-d04a-6432c2a415df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "f11 = f1_score(labels_of_test, predicted_label,average=None)\n",
        "re = recall_score(labels_of_test, predicted_label,average='weighted')\n",
        "cm = confusion_matrix(labels_of_test, predicted_label)\n",
        "pr_sc = precision_score(labels_of_test, predicted_label,average='weighted')\n",
        "clrp = classification_report(labels_of_test, predicted_label)\n",
        "print(\"accuracy score \",accu_score)\n",
        "print(\"********************\")\n",
        "print(\"f1 score \",f11)\n",
        "print(\"********************\")\n",
        "print(\"recall score \",re)\n",
        "print(\"********************\")\n",
        "print(\"confusion matrix \",cm)\n",
        "print(\"********************\")\n",
        "print(\"precision score \",pr_sc)\n",
        "print(\"********************\")\n",
        "print(\"classification report \",clrp)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy score  0.7867858282796042\n",
            "********************\n",
            "f1 score  [0.79774011 0.76965979 0.78767877]\n",
            "********************\n",
            "recall score  0.7867858282796042\n",
            "********************\n",
            "confusion matrix  [[1059  114  118]\n",
            " [ 168  690   61]\n",
            " [ 137   70  716]]\n",
            "********************\n",
            "precision score  0.7871846886602186\n",
            "********************\n",
            "classification report                precision    recall  f1-score   support\n",
            "\n",
            "         EAP       0.78      0.82      0.80      1291\n",
            "         HPL       0.79      0.75      0.77       919\n",
            "         MWS       0.80      0.78      0.79       923\n",
            "\n",
            "    accuracy                           0.79      3133\n",
            "   macro avg       0.79      0.78      0.79      3133\n",
            "weighted avg       0.79      0.79      0.79      3133\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}